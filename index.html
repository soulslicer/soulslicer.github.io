<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<html>

<!-- =========================== Header ============================ -->
<head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
  <meta name="generator" content="HTML Tidy for Linux/x86 (vers 11 February 2007), see www.w3.org">
  <link rel="stylesheet" href="fonts/fonts.css">
  <link rel="stylesheet" href="style/style.css">
  <script type="text/javascript" src="js/hidebib.js"></script>
  <title>Yaadhav Raaj</title>
  <meta name="Yaadhav Raaj&#39;s Homepage" http-equiv="Content-Type" content="Yaadhav Raaj&#39;s Homepage">
</head>

<!-- =========================== Body ============================ -->
<body>
  <table width="1080" border="0" align="center" cellspacing="0" cellpadding="20">
    <tbody>
      <tr>
        <td>

          <!-- =========================== Title ============================ -->
          <p align="center">
            <!-- <font size="7">Siddharth Ancha</font><br> -->
            <!-- use this colorpicker: https://imagecolorpicker.com/color-code/1772d0 -->
            <pageheading>Yaadhav <span style="color: #858585">Raaj</span></pageheading><br>
            perception • robotics • engineer
          </p>
          <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">

            <!-- =========================== Intro ============================ -->
            <tbody>
              <tr>
                <td width="33%" valign="top"><a href="images/headshot.jpg"><img src="images/headshot.jpg" width="100%"
                      style="border-radius:15px"></a>
                  <p align="center">
                    <a href="https://docs.google.com/viewer?url=https://docs.google.com/document/d/1P2RX_8Xw2fx0bkZSlHMLHE8j_IbW0hs5rmJxC75CmCw/export?format=pdf"><img src="images/cv.svg" alt="CV"
                        style="opacity:0.4;width:22px;height:22px;"></a>
                    &nbsp;
                    <a href="https://github.com/soulslicer"><img src="images/github.svg" alt="Github"
                        style="opacity:0.4;width:22px;height:22px;"></a>
                    &nbsp;
                    <a href="mailto:yaadhavraaj@gmail.com"><img src="images/gmail.svg" alt="Gmail"
                        style="opacity:0.4;width:22px;height:22px;"></a>
                    &nbsp;
                    <a href="https://scholar.google.com/citations?user=JaA7wpgAAAAJ&hl=en"><img src="images/google_scholar.svg" alt="Scholar"
                        style="opacity:0.4;width:22px;height:22px;"></a>
                    &nbsp;
                    <a href="https://www.linkedin.com/in/yaadhavraaj/"><img src="images/linkedin.svg" alt="Linkedin"
                        style="opacity:0.4;width:22px;height:22px;"></a>
                  </p>
                </td>
                <td width="69%" valign="top" align="justify">
                  <p>
                    Raaj here. I've been building computer vision systems for over a decade, for a variety of research, commercial and
                    industrial platforms, with multiple publications and patents in the field. Most recently, i've been building vision
                    systems for humanoid robots at <a href="https://agilityrobotics.com/" target="_blank">Agility Robotics</a>.
                  </p>
                  <p>
                    Prior to this, I was doing research at the <a href="https://www.ri.cmu.edu/" target="_blank">CMU Robotics
                      Institute</a>
                    working under the esteemed professors <a href="https://www.cs.cmu.edu/~srinivas/" target="_blank">Srinivasa
                      Narasimhan</a> and <a href="https://www.cs.cmu.edu/~yaser/" target="_blank">Yaser Sheikh</a> and earned a masters
                    along the way. Before that, I did a stint building vision systems for large scale industrial robots for the Singapore
                    Changi Airport with <a href="https://speedcargo.sg/automate/" target="_blank">SpeedCargo</a>. And prior to that, I was building vision systems for underwater robots with <a href="https://bumblebee.sg/" target="_blank">Bumblebee</a>.
                  </p>
                  <p>
                    My resume is linked right <a href="https://docs.google.com/viewer?url=https://docs.google.com/document/d/1P2RX_8Xw2fx0bkZSlHMLHE8j_IbW0hs5rmJxC75CmCw/export?format=pdf" target="_blank">here</a>
                  </p>
                </td>
              </tr>
            </tbody>
          </table>
          <hr>

          <!-- =========================== Education ============================ -->
          <table width="100%" align="center" border="0" cellspacing="0" cellpadding="10" style="padding-bottom: 10px;">
            <tbody>
              <tr>
                <td>
                  <sectionheading>&nbsp;&nbsp;Education</sectionheading>
                </td>
              </tr>
            </tbody>
          </table>
          <table width="100%" align="center" border="0" cellspacing="0" cellpadding="7.5px"
            style="padding-bottom: 0px;">
            <tbody>
              <tr>
                <td width="35%" valign="center" align="center" style="padding-bottom: 25px;">
                  <a href="https://www.cmu.edu/" target="_blank">
                    <img src="images/cmu.png" alt="image not found" height="81px">
                  </a>
                </td>
                <td width="65%" valign="center" align="left" style="padding-bottom: 25px;">
                  <i><b>Carnegie Mellon University</b></i><br>
                  Masters, Masters of Science in Robotics (MSR)<br>
                  CMU Robotics Institute<br>
                  <i><b>2018 ─ 2021</b></i>
                </td>
              </tr>
              <tr>
                <td width="35%" valign="center" align="center" style="padding-bottom: 20px;">
                  <a href="https://iitg.ac.in/" target="_blank">
                    <img src="images/nus.svg" alt="image not found" height="108px">
                  </a>
                </td>
                <td width="65%" valign="center" align="left" style="padding-left: 10px; padding-bottom: 20px;">
                  <i><b>National University of Singapore</b></i><br>
                  Bachelors, Computer Engineering<br>
                  Minor in Technopreneurship<br>
                  <i><b>2012 ─ 2016</b></i>
                </td>
              </tr>
            </tbody>
          </table>
          <hr>

          <!-- =========================== Career ============================ -->
          <table id="career" width="100%" align="center" border="0" cellspacing="0" cellpadding="10">
            <tbody>
              <tr>
                <td>
                  <sectionheading>&nbsp;&nbsp;Career</sectionheading>
                </td>
              </tr>
            </tbody>
          </table>

          <table id="career" width="100%" align="center" border="0" cellspacing="0" cellpadding="0">

          </table>

          <table width="100%" align="center" border="0" cellspacing="0" cellpadding="10">

            <tbody>
              <subheading>&nbsp;&nbsp;&nbsp;&nbsp;Agility Robotics - Portland, OR - 2021</subheading>

              <!-- =========================== Item 1============================ -->
              <tr class="pub">
                <td class="pub_thumbnail">
                  <a href="https://agilityrobotics.com/products/digit" target="_blank">
                    <img class="pub" src="images/agility-intro.jpg" alt="image not found"
                      width="93%">
                  </a>
                </td>
                <td class="pub_text">
                  <p><a href="https://www.youtube.com/watch?v=rnFZAB9ogEE" target="_blank">
                      <heading>Digit V4: A Mobile Manipulation Robot</heading>
                    </a>
                    <br>
                    First Perception Engineer to join Agility Robotics in Mid 2021.
                    Lead the development effort in setting up an object detection framework on Digit V4 from scratch.
                    Involved an end to end effort with a great team to build out a logging stack, GPU selection and testing, annotation vendor selection and integration, detection model selection and deployment, 6DOF object detection and tracking.
                    This system now drives the majority of our customer deployments.
                    <br>
                  </p>
                </td>
              </tr>

              <!-- =========================== Item 2 ============================ -->
              <tr class="pub">
                <td class="pub_thumbnail">
                  <a href="images/agility-obj.gif" target="_blank">
                    <img class="pub" src="images/agility-obj.gif" alt="image not found"
                      width="93%">
                  </a>
                </td>
                <td class="pub_text">
                  <p><a href="https://youtu.be/MhRd0vvdaj0?feature=shared&t=187" target="_blank">
                      <heading>6DOF Object Detection and Tracking</heading>
                    </a>
                    <br>
                    Helped develop the framework to do end to end multi object 6DOF pose detection and tracking.
                    This involved developing a model that was able to regress on contours, keypoints and pose, lifting those objects to 3D,
                    and tracking them with state estimation algorithms. These are done while leveraging the constraints of the
                    end-effectors. 
                    <br><span class="pub_venue">Patent Pending</span>
                    <br>
                  </p>
                </td>
              </tr>

              <!-- =========================== Item 3 ============================ -->
              <tr class="pub">
                <td class="pub_thumbnail">
                  <a href="images/agility-amazon.gif" target="_blank">
                    <img class="pub" src="images/agility-amazon.gif" alt="image not found"
                      width="93%">
                  </a>
                </td>
                <td class="pub_text">
                  <p><a href="https://agilityrobotics.com/solutions/tote-recycling" target="_blank">
                      <heading>Tote Recycling Product</heading>
                    </a>
                    <br>
                    Involved with Agility's <a href="https://www.youtube.com/watch?v=q8IdbodRG14" target="_blank">Amazon</a> deployments, including our Tote Recycling product workflow. This involved solving multiple customer specific problems, such as tote orientation, tote in hand and nested tote detection. 
                    <br>
                  </p>
                </td>
              </tr>

            </tbody>
          </table>

          <table width="100%" align="center" border="0" cellspacing="0" cellpadding="10">

            <tbody>
              <subheading>&nbsp;&nbsp;&nbsp;&nbsp;CMU Robotics Institute - Pittsburgh, PA - 2018 to 2021</subheading>

              <!-- =========================== Item 0 ============================ -->
              <tr class="pub">
                <td class="pub_thumbnail">
                  <a href="images/lc.png" target="_blank">
                    <img class="pub" src="images/lc.png" alt="image not found"
                      width="93%">
                  </a>
                </td>
                <td class="pub_text">
                  <p><a href="https://www.cs.cmu.edu/~ILIM/light_curtains/" target="_blank">
                      <heading>Triangulation Light Curtains</heading>
                    </a><br>
                    Worked at the <a href="https://www.cs.cmu.edu/~ILIM/" target="_blank">Illumination & Imaging Lab</a> at CMU, primarily
                    contributing towards projects relating to the Triangulation Light Curtain, a type of Adaptive LIDAR. I was involved in
                    a number of projects, including writing a simulator in C++ / CUDA for the imaging and dynamics, integrating a hardware
                    platform including cameras, a 360 LIDAR and this sensor with a full custom calibration suite, and a number of follow
                    up projects published at conferences.
                  </p>
                </td>
              </tr>

              <!-- =========================== Item 1 ============================ -->
              <tr class="pub">
                <td class="pub_thumbnail">
                  <a href="images/explot.gif" target="_blank">
                    <img class="pub" src="images/explot.gif" alt="image not found"
                      width="93%">
                  </a>
                </td>
                <td class="pub_text">
                  <p><a href="http://raaj.tech/rgb-lc-fusion/" target="_blank">
                      <heading>Exploiting & Refining Depth Distributions with Triangulation Light Curtains</heading>
                    </a><br>
                    <b>Yaadhav Raaj</b>, Siddharth Ancha, Robert Tamburo, David Held, Srinivasa Narasimhan<br>
                    <span class="pub_venue">CVPR 2021</span>
                  </p>
                  <div>
                    <a href="http://raaj.tech/rgb-lc-fusion/" target="_blank">webpage</a> |
                    <a href="javascript:toggleblock(&#39;exploit_abs&#39;)">abstract</a> |
                    <a href="https://openaccess.thecvf.com/content/CVPR2021/papers/Raaj_Exploiting__Refining_Depth_Distributions_With_Triangulation_Light_Curtains_CVPR_2021_paper.pdf" target="_blank">pdf</a> |
                    <a href="https://www.youtube.com/watch?v=kIjn3U8luV0" target="_blank">talk</a> |
                    <a href="https://github.com/CMU-Light-Curtains/DepthEstimation" target="_blank">code</a>
                    <p align="justify"> <i id="exploit_abs" style="display: none;">Active sensing through the use of Adaptive Depth Sensors
                        is a nascent field, with potential in areas such as Advanced driver-assistance systems (ADAS). They do however
                        require dynamically driving a laser / light-source to a specific location to capture information, with one such
                        class of sensor being the Triangulation Light Curtains (LC). In this work, we introduce a novel approach that
                        exploits prior depth distributions from RGB cameras to drive a Light Curtain's laser line to regions of uncertainty
                        to get new measurements. These measurements are utilized such that depth uncertainty is reduced and errors get
                        corrected recursively. We show real-world experiments that validate our approach in outdoor and driving settings,
                        and demonstrate qualitative and quantitative improvements in depth RMSE when RGB cameras are used in tandem with a
                        Light Curtain.</i></p>
                    <script xml:space="preserve" language="JavaScript">
                      hideblock('exploit_abs');
                    </script>
                  </div>
                </td>
              </tr>

              <!-- =========================== Item 2 ============================ -->
              <tr class="pub">
                <td class="pub_thumbnail">
                  <a href="images/active.gif" target="_blank">
                    <img class="pub" src="images/active.gif" alt="image not found"
                      width="93%">
                  </a>
                </td>
                <td class="pub_text">
                  <p><a href="https://siddancha.github.io/projects/active-perception-light-curtains/" target="_blank">
                      <heading>Active Perception using Light Curtains for Autonomous Driving</heading>
                    </a><br><b>Siddharth Ancha</b>, Yaadhav Raaj, Peiyun Hu, Srinivasa Narasimhan, David Held<br>
                    <span class="pub_venue">ECCV 2020</span>
                  </p>
                  <div>
                    <a href="https://siddancha.github.io/projects/active-perception-light-curtains/" target="_blank">webpage</a> |
                    <a href="javascript:toggleblock(&#39;active_abs&#39;)">abstract</a> |
                    <a href="https://arxiv.org/pdf/2008.02191" target="_blank">pdf</a> |
                    <a href="https://www.youtube.com/watch?v=WSb5T3HFE7w" target="_blank">talk</a> |
                    <a href="https://github.com/CMU-Light-Curtains/ObjectDetection" target="_blank">code</a>
                    <p align="justify"> <i id="active_abs" style="display: none;">Most real-world 3D sensors such as LiDARs perform fixed
                        scans of the entire environment, while being decoupled from the recognition system that processes the sensor data.
                        In this work, we propose a method for 3D object recognition using light curtains, a resource-efficient controllable
                        sensor that measures depth at user-specified locations in the environment. Crucially, we propose using prediction
                        uncertainty of a deep learning based 3D point cloud detector to guide active perception. Given a neural network's
                        uncertainty, we derive an optimization objective to place light curtains using the principle of maximizing
                        information gain. Then, we develop a novel and efficient optimization algorithm to maximize this objective by
                        encoding the physical constraints of the device into a constraint graph and optimizing with dynamic programming. We
                        show how a 3D detector can be trained to detect objects in a scene by sequentially placing uncertainty-guided light
                        curtains to successively improve detection accuracy.</i></p>
                    <script xml:space="preserve" language="JavaScript">
                      hideblock('active_abs');
                    </script>
                  </div>
                </td>
              </tr>

              <!-- =========================== Item 0 ============================ -->
              <tr class="pub">
                <td class="pub_thumbnail">
                  <a href="https://github.com/CMU-Perceptual-Computing-Lab/openpose" target="_blank">
                    <img class="pub" src="images/openpose.gif" alt="image not found"
                      width="93%">
                  </a>
                </td>
                <td class="pub_text">
                  <p><a href="https://github.com/CMU-Perceptual-Computing-Lab/openpose/" target="_blank">
                      <heading>OpenPose</heading>
                    </a><br>
                    Openpose is a vision software suite, that is capable of detecting full 2D human pose from a single image in real time, irrespective of the number of people in the scene. 
                    I was one of the core developers on the project at my time in the Perceptual Computing Lab, writing custom CUDA and OpenCL kernels, writing support for Windows / OSX, and developing a Python API.
                  </p>
                </td>
              </tr>

              <!-- =========================== Item 1 ============================ -->
              <tr class="pub">
                <td class="pub_thumbnail">
                  <a href="images/single.png" target="_blank">
                    <img class="pub" src="images/single.png" alt="image not found"
                      width="93%">
                  </a>
                </td>
                <td class="pub_text">
                  <p><a href="https://cmu-perceptual-computing-lab.github.io/spatio-temporal-affinity-fields/" target="_blank">
                      <heading>Single-Network Whole-Body Pose Estimation</heading>
                    </a><br>Gines Hidalgo, <b>Yaadhav Raaj</b>, Haroon Idrees, Donglai Xiang, Hanbyul Joo, Tomas Simon, Yaser Sheikh<br>
                    <span class="pub_venue">ICCV 2019</span>
                  </p>
                  <div>
                    <a href="https://paperswithcode.com/paper/single-network-whole-body-pose-estimation" target="_blank">webpage</a> |
                    <a href="javascript:toggleblock(&#39;single_abs&#39;)">abstract</a> |
                    <a href="https://openaccess.thecvf.com/content_ICCV_2019/papers/Hidalgo_Single-Network_Whole-Body_Pose_Estimation_ICCV_2019_paper.pdf" target="_blank">pdf</a> |
                    <a href="https://github.com/CMU-Perceptual-Computing-Lab/openpose_train" target="_blank">code</a>
                    <p align="justify"> <i id="single_abs" style="display: none;">We present the first single-network approach for 2D
                        whole-body pose estimation, which entails simultaneous localization of body, face, hands, and feet keypoints. Due to
                        the bottom-up formulation, our method maintains constant real-time performance regardless of the number of people in
                        the image. The network is trained in a single stage using multi-task learning, through an improved architecture
                        which can handle scale differences between body/foot and face/hand keypoints. Our approach considerably improves
                        upon OpenPose, the only work so far capable of whole-body pose estimation, both in terms of speed and global
                        accuracy. Unlike OpenPose, our method does not need to run an additional network for each hand and face candidate,
                        making it substantially faster for multi-person scenarios. This work directly results in a reduction of
                        computational complexity for applications that require 2D whole-body information (e.g., VR/AR, re-targeting). In
                        addition, it yields higher accuracy, especially for occluded, blurry, and low resolution faces and hands.</i></p>
                    <script xml:space="preserve" language="JavaScript">
                      hideblock('single_abs');
                    </script>
                  </div>
                </td>
              </tr>

              <!-- =========================== Item 2 ============================ -->
              <tr class="pub">
                <td class="pub_thumbnail">
                  <a href="images/staf.gif" target="_blank">
                    <img class="pub" src="images/staf.gif" alt="image not found"
                      width="93%">
                  </a>
                </td>
                <td class="pub_text">
                  <p><a href="https://cmu-perceptual-computing-lab.github.io/spatio-temporal-affinity-fields/" target="_blank">
                      <heading>Efficient Online Multi-Person 2D Pose Tracking with Recurrent Spatio-Temporal Affinity Fields</heading>
                    </a><br><b>Yaadhav Raaj</b>, Haroon Idrees, Gines Hidalgo, Yaser Sheikh<br>
                    <span class="pub_venue">CVPR 2019</span>
                  </p>
                  <div>
                    <a href="https://cmu-perceptual-computing-lab.github.io/spatio-temporal-affinity-fields/" target="_blank">webpage</a> |
                    <a href="javascript:toggleblock(&#39;staf_abs&#39;)">abstract</a> |
                    <a href="https://arxiv.org/pdf/1811.11975" target="_blank">pdf</a> |
                    <a href="https://www.youtube.com/watch?v=1Hg39MVNKBw" target="_blank">talk</a> |
                    <a href="https://github.com/soulslicer/STAF/tree/staf" target="_blank">code</a>
                    <p align="justify"> <i id="staf_abs" style="display: none;">We present an online approach to efficiently and
                        simultaneously detect and track 2D poses of multiple people in a video sequence. We build upon Part Affinity Fields
                        (PAF) representation designed for static images, and propose an architecture that can encode and predict
                        Spatio-Temporal Affinity Fields (STAF) across a video sequence. In particular, we propose a novel temporal topology
                        cross-linked across limbs which can consistently handle body motions of a wide range of magnitudes. Additionally, we
                        make the overall approach recurrent in nature, where the network ingests STAF heatmaps from previous frames and
                        estimates those for the current frame. Our approach uses only online inference and tracking, and is currently the
                        fastest and the most accurate bottom-up approach that is runtime-invariant to the number of people in the scene and
                        accuracy-invariant to input frame rate of camera. Running at $\sim$30 fps on a single GPU at single scale, it
                        achieves highly competitive results on the PoseTrack benchmarks.</i></p>
                    <script xml:space="preserve" language="JavaScript">
                      hideblock('staf_abs');
                    </script>
                  </div>
                </td>
              </tr>

            </tbody>
          </table>

          <table width="100%" align="center" border="0" cellspacing="0" cellpadding="10">

            <tbody>
              <subheading>&nbsp;&nbsp;&nbsp;&nbsp;SpeedCargo - Singapore - 2016 to 2018</subheading>

              <!-- =========================== Item 0 ============================ -->
              <tr class="pub">
                <td class="pub_thumbnail">
                  <a href="images/gudel.jpg" target="_blank">
                    <img class="pub" src="images/gudel.jpg" alt="image not found"
                      width="93%">
                  </a>
                </td>
                <td class="pub_text">
                  <p><a href="https://speedcargo.sg/automate/" target="_blank">
                      <heading>SpeedCargo</heading>
                    </a><br>
                    I spent two years being the primary vision developer for the SpeedCargo system, building the calibration, cargo
                    measurement, re-detection and large scale pallet scanning system using a custom Gudel industrial robot. Checkout our <a
                      href="https://www.youtube.com/watch?v=AUNNuvQ3mpk" target="_blank">video</a>
                  </p>
                </td>
              </tr>

              <!-- =========================== Item 1 ============================ -->
              <tr class="pub">
                <td class="pub_thumbnail">
                  <a href="images/ef.png" target="_blank">
                    <img class="pub" src="images/ef.png" alt="image not found"
                      width="93%">
                  </a>
                </td>
                <td class="pub_text">
                  <p><a href="https://speedcargo.sg/automate/" target="_blank">
                      <heading>Vision Driven End-Effector</heading>
                    </a><br>
                    Developed a custom multi-camera vision system for our custom tool changer, that could perform a high fidelity sub-mm 3D
                    scan of a very large workspace, and perform sub-mm positioning before grasping cargo. We developed a custom Basler
                    TOF-RGB <a href="images/basler.webp" target="_blank">system</a> to be integrated with this hardware, complete with its
                    own CUDA accelerated drivers, along with a unique calibration system leveraging the pose repeatability of the arm,
                    precision milled rods and calibration boards.
                  </p>
                </td>
              </tr>
            
              <!-- =========================== Item 2 ============================ -->
              <tr class="pub">
                <td class="pub_thumbnail">
                  <a href="images/accv.gif" target="_blank">
                    <img class="pub" src="images/accv.gif" alt="image not found"
                      width="93%">
                  </a>
                </td>
                <td class="pub_text">
                  <p><a href="https://link.springer.com/chapter/10.1007/978-3-319-54190-7_29" target="_blank">
                      <heading>Precise Measurement of Cargo Boxes for Gantry Robot Palletization in Large Scale Workspaces Using Low-Cost RGB-D Sensors</heading>
                    </a><br><b>Yaadhav Raaj</b>, Suraj Nair, Alois Knoll<br>
                    <span class="pub_venue">ACCV 2016</span>
                  </p>
                  <div>
                    <a href="javascript:toggleblock(&#39;staf_accv&#39;)">abstract</a> |
                    <a href="https://link.springer.com/chapter/10.1007/978-3-319-54190-7_29" target="_blank">pdf</a> |
                    <a href="https://youtu.be/VY9lebw-Qo8?feature=shared" target="_blank">talk</a>
                    <p align="justify"> <i id="staf_accv" style="display: none;">This paper presents a novel algorithm for extracting the
                        pose and dimensions of cargo boxes in a large measurement space of a robotic gantry, with sub-centimetre accuracy
                        using multiple low cost RGB-D Kinect sensors. This information is used by a bin-packing and path-planning software
                        to build up a pallet. The robotic gantry workspaces can be up to 10 m in all dimensions, and the cameras cannot be
                        placed top-down since the components of the gantry actuate within this space. This presents a challenge as occlusion
                        and sensor noise is more likely. This paper presents the system integration components on how point cloud
                        information is extracted from multiple cameras and fused in real-time, how primitives and contours are extracted and
                        corrected using RGB image features, and how cargo parameters from the cluttered cloud are extracted and optimized
                        using graph based segmentation and particle filter based techniques. This is done with sub-centimetre accuracy
                        irrespective of occlusion or noise from cameras at such camera placements and range to cargo.</i></p>
                    <script xml:space="preserve" language="JavaScript">
                      hideblock('staf_accv');
                    </script>
                  </div>
                </td>
              </tr>

              <!-- =========================== Item 3 ============================ -->
              <tr class="pub">
                <td class="pub_thumbnail">
                  <a href="images/constraints.gif" target="_blank">
                    <img class="pub" src="images/constraints.gif" alt="image not found"
                      width="93%">
                  </a>
                </td>
                <td class="pub_text">
                  <p><a href="https://link.springer.com/chapter/10.1007/978-3-319-54190-7_29" target="_blank">
                      <heading>Adapting the Search Subspace of a Particle Filter using Geometric Constraints
                      </heading>
                    </a><br>Nikhil Somani, <b>Yaadhav Raaj</b>, Suraj Nair, Alois Knoll<br>
                    <span class="pub_venue">BMVC Submission 2016</span>
                  </p>
                  <div>
                    <a href="javascript:toggleblock(&#39;staf_constraint&#39;)">abstract</a> |
                    <a href="https://mediatum.ub.tum.de/doc/1430196/580569.pdf" target="_blank">pdf</a> |
                    <a href="https://www.youtube.com/watch?v=TUJhq5Sz0xc&t=1s" target="_blank">talk</a>
                    <p align="justify"> <i id="staf_constraint" style="display: none;">Visual tracking of an object in 3D using its geometrical
                        model is an unsolved classical problem in computer vision. The use of point cloud (RGBD) data for likelihood
                        estimation in the state estimation loop provides improved matching as compared to 2D features. However, point cloud
                        processing is computationally expensive given its big data nature, making the use of mature tracking techniques such
                        as Particle Filters challenging. For practical applications, the filter requires implementation on hardware
                        acceleration platforms such as GPUs or FPGAs. In this paper, we introduce a novel approach for object tracking using
                        an adaptive Particle Filter operating on a point cloud based likelihood model. The novelty of the work comes from a
                        geometric constraint detection and solving system which helps reduce the search subspace of the Particle Filter. At
                        every time step, it detects geometric shape constraints and associates it with the object being tracked. Using this
                        information, it defines a new lower-dimensional search subspace for the state that lies in the nullspace of these
                        constraints. It also generates a new set of parameters for the dynamic model of the filter, its particle count and
                        the weights for multimodal fusion in the likelihood modal. As a consequence, it improves the convergence robustness
                        of the filter while reducing its computational complexity in the form of a reduced particle set.</i></p>
                    <script xml:space="preserve" language="JavaScript">
                      hideblock('staf_constraint');
                    </script>
                  </div>
                </td>
              </tr>

            </tbody>
          </table>

          <table width="100%" align="center" border="0" cellspacing="0" cellpadding="10">

            <tbody>
              <subheading>&nbsp;&nbsp;&nbsp;&nbsp;Bumblebee - Singapore - 2014 to 2016</subheading>

              <!-- =========================== Item 0 ============================ -->
              <tr class="pub">
                <td class="pub_thumbnail">
                  <a href="https://bumblebee.sg/auv/bbauv3/" target="_blank">
                    <img class="pub" src="images/bbauv.png" alt="image not found"
                      width="93%">
                  </a>
                </td>
                <td class="pub_text">
                  <p><a href="https://bumblebee.sg/auv/bbauv3/" target="_blank">
                      <heading>BBAUV 3.0</heading>
                    </a><br>
                    Spent time during my undergraduate period working on the BBAUV underwater robot platform, developing object
                    detection and tracking algorithms, by calibrating and fusing data from various sensor sources such as Sonar, Camera, IMU
                    and DVL. We won 2nd place in the AUVSI Robosub competition in San Diego in 2015, being the first team to complete the
                    torpedo board challenge in the history of the <a href="https://www.youtube.com/watch?v=09KNJ_uquw0&t=1s" target="_blank">competition</a>, thanks to this algorithm. 
                  </p>
                </td>
              </tr>

              <!-- =========================== Item 1 ============================ -->
              <tr class="pub">
                <td class="pub_thumbnail">
                  <a href="images/sonar-final.gif" target="_blank">
                    <img class="pub" src="images/sonar-final.gif" alt="image not found"
                      width="93%">
                  </a>
                </td>
                <td class="pub_text">
                  <p><a href="https://www.semanticscholar.org/paper/3D-Object-Localization-using-Forward-Looking-Sonar-Raaj-John/8814625a8287e161c8ae4a98a82e13b40619c029" target="_blank">
                      <heading>3D Object Localization using Forward Looking Sonar (FLS) and Optical Camera via particle filter based calibration and fusion
                      </heading>
                    </a><br><b>Yaadhav Raaj</b>, Alex John, Tan Jin<br>
                    <span class="pub_venue">IEEE Oceans 2016 - Patented</span>
                  </p>
                  <div>
                    <a href="javascript:toggleblock(&#39;ieee_constraint&#39;)">abstract</a> |
                    <a href="https://ieeexplore.ieee.org/document/7761077" target="_blank">pdf</a> |
                    <a href="https://www.youtube.com/watch?v=jU-1XNYjbcU&t=1s" target="_blank">talk</a> |
                    <a href="https://patents.google.com/patent/WO2021045679A1/en" target="_blank">patent</a>
                    <p align="justify"> <i id="ieee_constraint" style="display: none;">Underwater Object Localization is widely used in the
                        industry in Autonomous Underwater Vehicles (AUV), both in sea and lake environments for various applications. Sonars
                        and Cameras are popular choices for this, but each sensor alone poses several problems. Data extraction from Optical
                        Cameras underwater is a challenge due to poor lighting conditions, hazing over large distances and spatio-temporal
                        irradiate (flickering), while Sonars tend to have coarser sensor resolution and a lower signal-to-noise ratio (SNR)
                        making it difficult to extract data. This makes false positives more likely. In this paper, we present a robust
                        method to localize objects in front of an AUV in 3D space, using camera imagery, sonar imagery and odometry
                        information from onboard sensors. This is done through various image processing techniques, and a hybrid
                        sonar/camera particle filter based calibration step and fusion step.</i></p>
                    <script xml:space="preserve" language="JavaScript">
                      hideblock('ieee_constraint');
                    </script>
                  </div>
                </td>
              </tr>

              <!-- =========================== Item 2 ============================ -->
              <tr class="pub">
                <td class="pub_thumbnail">
                  <a href="https://bumblebee.sg/asv/bbasv/" target="_blank">
                    <img class="pub" src="images/bbasv.png" alt="image not found"
                      width="93%">
                  </a>
                </td>
                <td class="pub_text">
                  <p><a href="https://bumblebee.sg/auv/bbauv3/" target="_blank">
                      <heading>BBASV 1.0</heading>
                    </a><br>
                    Spent time during my undergraduate period working on the BBASV water surface vessel robot platform, developing object
                    detection and tracking algorithms, by calibrating and fusing data from various sensor sources such as Lidar, Cameras and
                    an IMU. We won 2nd place in the AUVSI Robosub <a href="https://www.youtube.com/watch?v=oYBuMMV1ZGk&t=1s"
                      target="_blank">competition</a> in Hawaii in 2016. More details on this <a href="https://bumblebee.sg/pdf/NUS_Bumblebee_2016_RobotX_Journal.pdf"
                      target="_blank">craft</a>.
                  </p>
                </td>
              </tr>

            </tbody>
          </table>

          <hr style="margin-top: 20px;">
          
          <!-- =========================== Other ============================ -->
          <table width="100%" align="center" border="0" cellpadding="10">
            <tbody>
              <tr>
                <td>
                  <sectionheading>&nbsp;&nbsp;Other</sectionheading>
                  <p style="margin-left: 10px;">
                    Being a vision person, i quite enjoy photography as a side hobby. And living in the Pacific Northwest, I enjoy imbibing
                    in the great outdoors, with a little hiking, rock climbing, and some mountaineering. I enjoy keeping up to date with all
                    the new technology in our ever evolving field, and most recently built a maze solver conditioned on images with a
                    diffusion based approach. Checkout my github for more projects.
                  </p>
                </td>
              </tr>
            </tbody>
          </table>
          <hr>

          <table width="100%" align="center" border="0" cellspacing="0" cellpadding="2">
            <tbody>
              <tr>
                <td>
                  <p align="right">
                    <font size="1.5">
                      Template modified from <a href="https://www.cs.cmu.edu/~dpathak/" target="_blank">this</a>
                    </font>
                  </p>
                </td>
              </tr>
            </tbody>
          </table>

        </td>
      </tr>
    </tbody>
  </table>
  <script xml:space="preserve" language="JavaScript">
    hideallbibs();
  </script>



</body>

</html>
